name: Test and Coverage

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: read
    strategy:
      matrix:
        python-version: ["3.9", "3.12"]

    steps:
      - uses: actions/checkout@v5
        with:
          submodules: recursive
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"
          cache-dependency-path: "pyproject.toml"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake ninja-build pkg-config libdbus-1-dev libglib2.0-dev libudev-dev libbluetooth-dev python3-dev

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,test,examples]

      - name: Run tests with coverage
        run: |
          python -m pytest tests/ -n auto --ignore=tests/benchmarks/ --junitxml=test-results.xml --cov=src/bluetooth_sig --cov-report=html --cov-report=xml --cov-report=term-missing --cov-fail-under=70

      - name: Publish JUnit test report
        if: always()
        uses: dorny/test-reporter@v2
        with:
          name: Pytest
          path: test-results.xml
          reporter: java-junit
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract coverage percentage and create badge
        if: matrix.python-version == '3.12'
        run: |
          python scripts/extract_coverage_badge.py

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test-results.xml
          retention-days: 30

      - name: Upload coverage artifacts
        if: matrix.python-version == '3.12'
        uses: actions/upload-artifact@v5
        with:
          name: coverage-report
          path: htmlcov
          retention-days: 30

  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      contents: read
      pull-requests: write

    steps:
      - uses: actions/checkout@v5
        with:
          submodules: recursive
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: "pyproject.toml"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake ninja-build pkg-config libdbus-1-dev libglib2.0-dev libudev-dev libbluetooth-dev python3-dev

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"

      - name: Set consistent Python hash seed
        run: echo "PYTHONHASHSEED=0" >> $GITHUB_ENV

      - name: Run benchmarks
        run: |
          export PYTHONPATH=$GITHUB_WORKSPACE/src:$PYTHONPATH
          python -m pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark.json \
            --benchmark-columns=min,max,mean,stddev \
            --benchmark-sort=name

      - name: Store benchmark baseline (push)
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: benchmark-action/github-action-benchmark@v1.20.7
        with:
          name: "Python Benchmarks"
          tool: "pytest"
          output-file-path: benchmark.json
          external-data-json-path: ./cache/benchmark-data.json
          save-data-file: true
          auto-push: false
        # don't require a token when not pushing pages

      - name: Upload benchmark baseline cache
        uses: actions/cache@v4
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        with:
          path: ./cache
          key: ${{ runner.os }}-benchmark

      - name: Download previous benchmark data
        uses: actions/cache@v4
        if: github.event_name == 'pull_request'
        with:
          path: ./cache
          key: ${{ runner.os }}-benchmark

      - name: Check for previous baseline
        if: github.event_name == 'pull_request'
        id: baseline_check
        run: |
          if [ -f ./cache/benchmark-data.json ]; then
            echo "found=true" >> $GITHUB_OUTPUT
          else
            echo "found=false" >> $GITHUB_OUTPUT
          fi

      - name: Debug benchmark JSON files (print summaries)
        if: github.event_name == 'pull_request' && steps.baseline_check.outputs.found == 'true'
        run: |
          echo "--- current benchmark.json ---"
          python - <<'PY'
          import json, sys
          try:
              d = json.load(open('benchmark.json'))
              print('type:', type(d).__name__)
              if isinstance(d, dict):
                  print('keys:', list(d.keys())[:20])
              elif isinstance(d, list):
                  print('len:', len(d))
                  if len(d):
                      print('sample keys:', list(d[0].keys()))
          except Exception as e:
              print('error reading benchmark.json', e)
          PY
          echo "--- cached ./cache/benchmark-data.json ---"
          python - <<'PY'
          import json, sys
          try:
              d = json.load(open('./cache/benchmark-data.json'))
              print('type:', type(d).__name__)
              if isinstance(d, dict):
                  print('top-level keys:', list(d.keys())[:20])
                  # if action format stores benchmarks under names, show those
                  for k in list(d.keys())[:5]:
                      v = d[k]
                      print(k, '-> type', type(v).__name__, ('len', len(v)) if hasattr(v, '__len__') else '')
              elif isinstance(d, list):
                  print('len list:', len(d))
                  if len(d):
                      print('sample keys:', list(d[0].keys()))
          except Exception as e:
              print('error reading cache file', e)
          PY

      - name: Compare with baseline
        if: github.event_name == 'pull_request' && steps.baseline_check.outputs.found == 'true'
        uses: benchmark-action/github-action-benchmark@v1.20.7
        with:
          name: "Python Benchmarks"
          tool: "pytest"
          output-file-path: benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          external-data-json-path: ./cache/benchmark-data.json
          alert-threshold: "200%"
          comment-on-alert: true
          fail-on-alert: true
          summary-always: true

      - name: Download previous benchmark history
        uses: dawidd6/action-download-artifact@v9
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        continue-on-error: true
        with:
          name: benchmark-history
          workflow: test-coverage.yml
          branch: main
          path: benchmark-history

      - name: Update benchmark history
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          python scripts/update_benchmark_history.py \
            benchmark.json \
            benchmark-history/history.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@v5
        with:
          name: benchmark-results
          path: benchmark.json
          retention-days: 90

      - name: Upload benchmark history
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v5
        with:
          name: benchmark-history
          path: benchmark-history/history.json
          retention-days: 90

  build-docs:
    name: Build Documentation
    needs: [test, benchmark]
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v5
        with:
          submodules: recursive
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"
          cache: "pip"
          cache-dependency-path: "pyproject.toml"

      - name: Install documentation dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[docs]"

      - name: Install system packages required for UML generation
        run: |
          # PlantUML requires Java; pydeps uses Graphviz (dot) to render
          # dependency graphs. Install both so docs generation is reliable.
          sudo apt-get update
          sudo apt-get install -y graphviz plantuml openjdk-11-jre-headless

      - name: Download coverage artifacts
        uses: actions/download-artifact@v6
        with:
          name: coverage-report
          path: htmlcov
        continue-on-error: true

      - name: Download benchmark artifacts
        uses: actions/download-artifact@v6
        with:
          name: benchmark-results
          path: benchmarks
        continue-on-error: true

      - name: Download benchmark history
        uses: dawidd6/action-download-artifact@v9
        continue-on-error: true
        with:
          name: benchmark-history
          workflow: test-coverage.yml
          branch: main
          path: benchmarks

      - name: Link coverage into docs directory
        run: |
          if [ -d "htmlcov" ]; then
            echo "✅ Coverage reports found, linking to docs/"
            rm -f docs/coverage
            ln -sf ../htmlcov docs/coverage
            ls -la docs/coverage | head -3
          else
            echo "⚠️ No coverage reports found, docs will build without coverage"
          fi

      - name: Link benchmarks into docs directory
        run: |
          if [ -f "benchmarks/benchmark.json" ]; then
            echo "✅ Benchmark results found, linking to docs/"
            mkdir -p docs/benchmarks
            cp benchmarks/benchmark.json docs/benchmarks/
            ls -la docs/benchmarks/
          else
            echo "⚠️ No benchmark results found, docs will build without benchmarks"
          fi

      - name: Build documentation
        run: |
          mkdocs build

      - name: Upload combined site artifact (main branch only)
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v5
        with:
          name: site
          path: site/
          retention-days: 30

  deploy:
    name: Deploy to GitHub Pages
    needs: build-docs
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
    steps:
      - name: Download site artifacts
        uses: actions/download-artifact@v6
        with:
          name: site
          path: site

      - name: Upload to GitHub Pages
        uses: actions/upload-pages-artifact@v4
        with:
          path: site

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
